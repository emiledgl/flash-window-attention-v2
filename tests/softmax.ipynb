{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "@triton.jit\n",
    "def _softmax_fwd_kernel(\n",
    "    out_ptr,\n",
    "    stride_out_row,\n",
    "    x_ptr,\n",
    "    stride_x_row,\n",
    "    num_cols: tl.constexpr,\n",
    "    block_size: tl.constexpr,\n",
    "):\n",
    "    row_index = tl.program_id(0)\n",
    "    row_start_ptr = x_ptr + row_index * stride_x_row\n",
    "    col_offsets = tl.arange(0, block_size)\n",
    "    row_mask = col_offsets < num_cols\n",
    "\n",
    "    # Move to SRAM\n",
    "    row = tl.load(row_start_ptr + col_offsets, mask=row_mask, other=float('-inf'))\n",
    "\n",
    "    # Softmax\n",
    "    safe_row = row - tl.max(row, axis=0)\n",
    "    numerator = tl.exp(safe_row)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    sm_out = numerator / denominator\n",
    "\n",
    "    # Write back to HBM\n",
    "    out_row_ptr = out_ptr + row_index * stride_out_row\n",
    "    tl.store(out_row_ptr + col_offsets, sm_out, mask=row_mask)\n",
    "    \n",
    "def softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Triton implementation of Softmax\"\"\"\n",
    "\n",
    "    assert x.dim() == 2, \"Only accepts 2D tensors\"\n",
    "    rows, cols = x.shape\n",
    "    sm_out = torch.empty_like(x)\n",
    "    \n",
    "    BLOCK_SIZE = triton.next_power_of_2(cols)\n",
    "    MAX_WARPS = 16\n",
    "    num_warps = min(2 ** (2 + BLOCK_SIZE // 2048), MAX_WARPS)\n",
    "    grid = (rows,)\n",
    "\n",
    "    _softmax_fwd_kernel[grid](\n",
    "        sm_out,\n",
    "        sm_out.stride(0),\n",
    "        x,\n",
    "        x.stride(0),\n",
    "        cols,\n",
    "        block_size=BLOCK_SIZE,\n",
    "        num_warps=num_warps,\n",
    "    )\n",
    "\n",
    "    return sm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0117, 0.0317, 0.0861, 0.2341, 0.6364], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.Tensor([\n",
    "    [1,2,3,4,5],\n",
    "    [5,4,3,2,1],\n",
    "    ])\n",
    "x = x.to(dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "row0 = x[0]\n",
    "max = torch.max(row0, dim=0)[0]\n",
    "safe_row0 = row0 - max\n",
    "sm_out = torch.exp(safe_row0)\n",
    "sm_out /= torch.sum(sm_out, dim=0)\n",
    "sm_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
